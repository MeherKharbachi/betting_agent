{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp simple_agent.agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from IPython.core.debugger import set_trace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Agent\n",
    ">  Train a simple agent to bet on football games "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "import d3rlpy\n",
    "import pandas as pd\n",
    "import torch\n",
    "from betting_env.betting_env import BettingEnv\n",
    "from betting_env.utils.data_extractor import *\n",
    "from torch.optim import Adam\n",
    "\n",
    "from betting_agent.config.localconfig import CONFIG, DB_HOSTS\n",
    "from betting_agent.patching.monkey_patching import *\n",
    "from betting_agent.patching.uncache import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixtures = data_aggregator(\n",
    "    db_hosts=DB_HOSTS, config=CONFIG, db_host=\"prod_atlas\", limit=4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Monkey-patching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "from d3rlpy import torch_utility\n",
    "from d3rlpy.online.buffers import ReplayBuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "torch_utility.torch_api = torch_api\n",
    "ReplayBuffer.append = append\n",
    "ReplayBuffer._add_last_step = add_last_step\n",
    "uncache([\"d3rlpy.torch_utility\", \"d3rlpy.online.buffers\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "from d3rlpy.algos import DQN\n",
    "from d3rlpy.algos.base import AlgoBase\n",
    "from d3rlpy.models.optimizers import OptimizerFactory\n",
    "from d3rlpy.online.explorers import LinearDecayEpsilonGreedy\n",
    "from d3rlpy.preprocessing.scalers import Scaler\n",
    "\n",
    "AlgoBase.fit_online = fit_online\n",
    "\n",
    "from betting_agent.simple_agent.network_architecture import *\n",
    "from betting_agent.simple_agent.scaler import SimpleScaler\n",
    "\n",
    "uncache([\"d3rlpy.torch_utility\", \"d3rlpy.online.buffers\", \"d3rlpy.algos.base\", \"d3rlpy.preprocessing.scalers\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We propose a function that will prepare the `Reinforcement learning` algorithm prior to training. Initially, we initialise the `Betting environment` with the supplied data, then we set up the `Scaler`, which will transform our observations to particular features from the Database, and last, we set up the `Buffer`; `D3rlpy` supports both offline and online training tools. In this case, the `Buffer` will try several experiences in order to obtain a useful dataset.\n",
    "\n",
    "Furthemore, we supply additionally an `Optimizer` to update weights and reduce losses for the `Neural Network` and an `Explorer` which will apply the `exploration-exploitation` dilemma which must exist side by side because The majority of the time, the `epsilon-greedy` strategy takes the action with the largest estimated reward. `Exploration` allows us to experiment with new ideas, which are frequently at contradiction with what we have already learned. The procedure starts with 100% `exploration` and subsequently decreases to 10%.\n",
    "\n",
    "We should note that the `D3rlpy` package has several `RL` algorithms; in our situation, we will choose the `DQN` algorithm (Deep Q-Network)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "def rl_algo_preparation(\n",
    "    fixtures: pd.DataFrame,  # All provided games.\n",
    "    algo: d3rlpy.algos,  # D3rlpy RL algorithm.\n",
    "    algo_batch_size=32,  #  Mini-batch size.\n",
    "    algo_learning_rate=2.5e-4,  # Algo learning rate.\n",
    "    algo_target_update_interval=100,  # Interval to update the target network.\n",
    "    algo_scaler: str = \"simple\", # name of the scaler to apply\n",
    "    optimizer: torch.optim = Adam,  # Algo Optimizer.\n",
    "    optimizer_weight_decay=1e-4,  # Optimizer weight decay.\n",
    "    maxlen_buffer=1000000,  #  The maximum number of data length.\n",
    "    explorer_start_epsilon=1.0,  # The beginning epsilon.\n",
    "    explorer_end_epsilon=0.1,  # The end epsilon.\n",
    "    explorer_duration=100000,  # The scheduling duration.\n",
    "    **kwargs,  # extra arguments accepted by `SimpleEncoderFactory()`\n",
    "):\n",
    "    \"Prepare RL algorithm components.\"\n",
    "    # Init betting env.\n",
    "    env = BettingEnv(fixtures)\n",
    "    \n",
    "    # scaler\n",
    "    scaler = SimpleScaler()\n",
    "\n",
    "    # Init Buffer.\n",
    "    buffer = ReplayBuffer(env=env, maxlen=maxlen_buffer)\n",
    "\n",
    "    # Init the epsilon-greedy explorer\n",
    "    explorer = LinearDecayEpsilonGreedy(\n",
    "        start_epsilon=explorer_start_epsilon,\n",
    "        end_epsilon=explorer_end_epsilon,\n",
    "        duration=explorer_duration,\n",
    "    )\n",
    "\n",
    "    # Init Optimizer.\n",
    "    optim_factory = OptimizerFactory(optimizer, weight_decay=optimizer_weight_decay)\n",
    "\n",
    "    # Init RL Algo.\n",
    "    rl_algo = algo(\n",
    "        batch_size=algo_batch_size,\n",
    "        learning_rate=algo_learning_rate,\n",
    "        target_update_interval=algo_target_update_interval,\n",
    "        optim_factory=optim_factory,\n",
    "        scaler=scaler,\n",
    "        encoder_factory=SimpleEncoderFactory(\n",
    "            feature_size=env.action_space.n,\n",
    "            observation_size=scaler.OBS,\n",
    "            **kwargs,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    return env, buffer, explorer, rl_algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def launch_training(\n",
    "    fixtures: pd.DataFrame,  # All provided games.\n",
    "    algo: d3rlpy.algos,  # D3rlpy RL algorithm.\n",
    "    training_steps: int = 100,  # The number of total steps to train.\n",
    "    n_steps_per_epoch: int = 50,  # The number of steps per epoch.\n",
    "    update_start_step: int = 50,  #  The steps before starting updates.\n",
    "    algo_batch_size: int = 32,  #  Mini-batch size.\n",
    "    algo_learning_rate: float = 2.5e-4,  # Algo learning rate.\n",
    "    algo_target_update_interval: int = 100,  # Interval to update the target network.\n",
    "    algo_scaler: Scaler = SimpleScaler,  # The scaler for data transformation.\n",
    "    optimizer: torch.optim = Adam,  # Algo Optimizer.\n",
    "    optimizer_weight_decay: float = 1e-4,  # Optimizer weight decay.\n",
    "    maxlen_buffer: int = 1000000,  #  The maximum number of data length.\n",
    "    explorer_start_epsilon: float = 1.0,  # The beginning epsilon.\n",
    "    explorer_end_epsilon: float = 0.1,  # The end epsilon.\n",
    "    explorer_duration: int = 100,  # The scheduling duration.\n",
    "    eval_epsilon: float = 0.3,  # Greedy-epsilon for evaluation.\n",
    "    show_progress: bool = True,  # Flag to show progress bar for iterations.\n",
    "    save_metrics: bool = True,  # Flag to record metrics. If False, the log directory is not created and the model parameters are not saved.\n",
    "):\n",
    "    \"Launch RL algorithm training.\"\n",
    "    # Get algo params.\n",
    "    env, buffer, explorer, rl_algo = rl_algo_preparation(\n",
    "        fixtures=fixtures,\n",
    "        algo=algo,\n",
    "        algo_batch_size=algo_batch_size,\n",
    "        algo_learning_rate=algo_learning_rate,\n",
    "        algo_target_update_interval=algo_target_update_interval,\n",
    "        algo_scaler=algo_scaler,\n",
    "        optimizer=optimizer,\n",
    "        optimizer_weight_decay=optimizer_weight_decay,\n",
    "        maxlen_buffer=maxlen_buffer,\n",
    "        explorer_start_epsilon=explorer_start_epsilon,\n",
    "        explorer_end_epsilon=explorer_end_epsilon,\n",
    "        explorer_duration=explorer_duration,\n",
    "    )\n",
    "    # Launch training.\n",
    "    eval_env = BettingEnv(fixtures)\n",
    "    rl_algo.fit_online(\n",
    "        env,  # Gym environment.\n",
    "        buffer,  # Buffer.\n",
    "        explorer,  # Explorer.\n",
    "        n_steps=training_steps,  # Train for 'training_steps' steps.\n",
    "        n_steps_per_epoch=n_steps_per_epoch,  # Evaluation is performed every 'n_steps_per_epoch' steps.\n",
    "        update_start_step=update_start_step,  # Parameter update starts after 'update_start_step' steps.\n",
    "        save_metrics=save_metrics,  # Save metrics.\n",
    "        show_progress=show_progress,  # Show progress.\n",
    "        eval_env=eval_env,  # Environment for evaluation.\n",
    "        eval_epsilon=eval_epsilon,  # Greedy-epsilon for evaluation.\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-10 07:45.14 [info     ] Directory is created at d3rlpy_logs/DQN_online_20230310074514\n",
      "2023-03-10 07:45.14 [debug    ] Fitting scaler...              scler=simple\n",
      "2023-03-10 07:45.14 [debug    ] Building model...\n",
      "2023-03-10 07:45.14 [debug    ] Model has been built.\n",
      "2023-03-10 07:45.14 [info     ] Parameters are saved to d3rlpy_logs/DQN_online_20230310074514/params.json params={'action_scaler': None, 'batch_size': 32, 'encoder_factory': {'type': 'custom', 'params': {'feature_size': 16}}, 'gamma': 0.99, 'generated_maxlen': 100000, 'learning_rate': 0.00025, 'n_critics': 1, 'n_frames': 1, 'n_steps': 1, 'optim_factory': {'optim_cls': 'Adam', 'weight_decay': 0.0001}, 'q_func_factory': {'type': 'mean', 'params': {'share_encoder': False}}, 'real_ratio': 1.0, 'reward_scaler': None, 'scaler': {'type': 'simple', 'params': {}}, 'target_update_interval': 100, 'use_gpu': None, 'algorithm': 'DQN', 'observation_shape': (30,), 'action_size': 16}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df511664092b4c338c888788e41a9803",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-10 07:45.23 [info     ] Model parameters are saved to d3rlpy_logs/DQN_online_20230310074514/model_20.pt\n",
      "2023-03-10 07:45.23 [info     ] DQN_online_20230310074514: epoch=1 step=20 epoch=1 metrics={'time_inference': 0.1550007939338684, 'time_environment_step': 0.0003853321075439453, 'time_step': 0.267804217338562, 'rollout_return': -50.48, 'evaluation': 8.935000000000006} step=20\n",
      "2023-03-10 07:45.30 [info     ] Model parameters are saved to d3rlpy_logs/DQN_online_20230310074514/model_40.pt\n",
      "2023-03-10 07:45.30 [info     ] DQN_online_20230310074514: epoch=2 step=40 epoch=2 metrics={'time_inference': 0.10758460760116577, 'time_environment_step': 0.00040752887725830077, 'time_step': 0.21772069931030275, 'rollout_return': 20.270000000000003, 'time_sample_batch': 0.00010214533124651228, 'time_algorithm_update': 0.004025254930768695, 'loss': 54.871473039899556, 'evaluation': -42.795} step=40\n",
      "2023-03-10 07:45.38 [info     ] Model parameters are saved to d3rlpy_logs/DQN_online_20230310074514/model_60.pt\n",
      "2023-03-10 07:45.38 [info     ] DQN_online_20230310074514: epoch=3 step=60 epoch=3 metrics={'time_inference': 0.12068487405776977, 'time_environment_step': 0.0004119873046875, 'time_sample_batch': 0.00010592937469482422, 'time_algorithm_update': 0.0017395377159118651, 'loss': 49.775203704833984, 'time_step': 0.23742103576660156, 'rollout_return': 17.380000000000003, 'evaluation': 25.015000000000004} step=60\n",
      "2023-03-10 07:45.45 [info     ] Model parameters are saved to d3rlpy_logs/DQN_online_20230310074514/model_80.pt\n",
      "2023-03-10 07:45.45 [info     ] DQN_online_20230310074514: epoch=4 step=80 epoch=4 metrics={'time_inference': 0.10515484809875489, 'time_environment_step': 0.000435638427734375, 'time_sample_batch': 0.00010796785354614258, 'time_algorithm_update': 0.0018878817558288575, 'loss': 42.17471036911011, 'time_step': 0.2168915390968323, 'rollout_return': -6.700000000000003, 'evaluation': -24.485000000000003} step=80\n",
      "2023-03-10 07:45.52 [info     ] Model parameters are saved to d3rlpy_logs/DQN_online_20230310074514/model_100.pt\n",
      "2023-03-10 07:45.52 [info     ] DQN_online_20230310074514: epoch=5 step=100 epoch=5 metrics={'time_inference': 0.10567585229873658, 'time_environment_step': 0.00042592287063598634, 'time_sample_batch': 0.0001034379005432129, 'time_algorithm_update': 0.0018742918968200684, 'loss': 42.04249935150146, 'time_step': 0.219627845287323, 'rollout_return': -9.400000000000002, 'evaluation': -17.825000000000003} step=100\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "launch_training(\n",
    "    fixtures=fixtures,\n",
    "    algo=DQN,\n",
    "    algo_scaler=SimpleScaler,\n",
    "    optimizer=Adam,\n",
    "    explorer_duration=100,\n",
    "    training_steps=100,\n",
    "    n_steps_per_epoch=20,\n",
    "    update_start_step=20,\n",
    "    save_metrics=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev import nbdev_export\n",
    "\n",
    "nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
