{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp Agent.agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from IPython.core.debugger import set_trace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# D3rlpy Agent\n",
    "\n",
    ">  Simulate a trading strategy using a custom football betting environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "import pandas as pd\n",
    "import d3rlpy\n",
    "import torch\n",
    "from betting_agent.Utils.uncache import *\n",
    "from betting_agent.Utils.monkey_patching import *\n",
    "from d3rlpy.preprocessing.scalers import Scaler\n",
    "from betting_agent.config.localconfig import CONFIG, DB_HOSTS\n",
    "from betting_env.betting_env import BettingEnv\n",
    "from betting_env.utils.data_extractor import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gameId</th>\n",
       "      <th>game_optaId</th>\n",
       "      <th>gameDate</th>\n",
       "      <th>homeTeamId</th>\n",
       "      <th>homeTeam_optaId</th>\n",
       "      <th>awayTeamId</th>\n",
       "      <th>awayTeam_optaId</th>\n",
       "      <th>tgt_gd</th>\n",
       "      <th>tgt_outcome</th>\n",
       "      <th>preGameOdds1</th>\n",
       "      <th>...</th>\n",
       "      <th>homeTeamLineupIds</th>\n",
       "      <th>homeTeamLineupSlots</th>\n",
       "      <th>homeTeamFormation</th>\n",
       "      <th>home_team_lineup_received_at</th>\n",
       "      <th>awayTeamName</th>\n",
       "      <th>awayTeamLineup</th>\n",
       "      <th>awayTeamLineupIds</th>\n",
       "      <th>awayTeamLineupSlots</th>\n",
       "      <th>awayTeamFormation</th>\n",
       "      <th>away_team_lineup_received_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>d0cc49c3230e300b529b270951b3b70b3224481add8354...</td>\n",
       "      <td>991007</td>\n",
       "      <td>2018-08-21 18:45:00</td>\n",
       "      <td>9ca1f9a87934693b07890de4b4528b0f3ae4065a67ec38...</td>\n",
       "      <td>80</td>\n",
       "      <td>38ca605bcd29a5a37697ca66e533ae817ced71b6bf275c...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.13</td>\n",
       "      <td>...</td>\n",
       "      <td>[40346, 28654, 49539, 169432, 214225, 116215, ...</td>\n",
       "      <td>[1, 3, 2, 9, 6, 11, 7, 5, 4, 8, 10]</td>\n",
       "      <td>4-2-3-1</td>\n",
       "      <td>2018-08-21 18:15:00</td>\n",
       "      <td>Leeds United</td>\n",
       "      <td>{\"Kalvin Phillips\": \"DMC\", \"Jamie Shackleton\":...</td>\n",
       "      <td>[155405, 221610, 98760, 57913, 220037, 38588, ...</td>\n",
       "      <td>[4, 2, 9, 3, 1, 5, 7, 8, 10, 11, 6]</td>\n",
       "      <td>4-1-4-1</td>\n",
       "      <td>2018-08-21 18:15:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c0c48eee0b1a42e0d84cb0a947fe2c64f9e1aa7015922f...</td>\n",
       "      <td>990998</td>\n",
       "      <td>2018-08-21 18:45:00</td>\n",
       "      <td>bc9d5de208258f2f95282c59e9551310be9d319ebc6e4e...</td>\n",
       "      <td>24</td>\n",
       "      <td>4a625f945d8f58984be0aa7b2ac6409a23ed9cf48e4260...</td>\n",
       "      <td>40</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.05</td>\n",
       "      <td>...</td>\n",
       "      <td>[106423, 12744, 184341, 57714, 103920, 17601, ...</td>\n",
       "      <td>[4, 9, 8, 3, 10, 1, 2, 5, 6, 11, 7]</td>\n",
       "      <td>4-3-3</td>\n",
       "      <td>2018-08-21 18:15:00</td>\n",
       "      <td>Ipswich Town</td>\n",
       "      <td>{\"Jonas Knudsen\": \"DL\", \"Janoi Donacien\": \"DR\"...</td>\n",
       "      <td>[82187, 154936, 101881, 115557, 28530, 19910, ...</td>\n",
       "      <td>[3, 2, 10, 7, 1, 8, 9, 11, 4, 5, 6]</td>\n",
       "      <td>4-1-4-1</td>\n",
       "      <td>2018-08-21 18:15:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>58b1242154c8055252582229abfc4680460278834c4433...</td>\n",
       "      <td>991001</td>\n",
       "      <td>2018-08-21 18:45:00</td>\n",
       "      <td>58301066042bbdf19de8fe7d41afc53626b5aa79034712...</td>\n",
       "      <td>72</td>\n",
       "      <td>bbb63e4ea54b0d60b48a1f8440254d7e656dfbfcbef825...</td>\n",
       "      <td>88</td>\n",
       "      <td>-1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.09</td>\n",
       "      <td>...</td>\n",
       "      <td>[80246, 19152, 193576, 155529, 124120, 41753, ...</td>\n",
       "      <td>[7, 6, 2, 1, 4, 3, 11, 8, 5, 9, 10]</td>\n",
       "      <td>4-5-1</td>\n",
       "      <td>2018-08-21 18:15:00</td>\n",
       "      <td>Hull City</td>\n",
       "      <td>{\"Evandro Goebel\": \"AMC\", \"Jordy De Wijs\": \"DC...</td>\n",
       "      <td>[52287, 173549, 120449, 28541, 107692, 178186,...</td>\n",
       "      <td>[10, 6, 2, 9, 11, 7, 4, 5, 3, 1, 8]</td>\n",
       "      <td>4-4-1-1</td>\n",
       "      <td>2018-08-21 18:15:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3a604f5616b39eb17fc8d1eed07d5248e387bf400294b2...</td>\n",
       "      <td>991000</td>\n",
       "      <td>2018-08-21 18:45:00</td>\n",
       "      <td>e2bfbb5453a7853e049b9434db74d4d06b8c5560ff7cf9...</td>\n",
       "      <td>52</td>\n",
       "      <td>d6fe4a4ffbf1e1a0ae9d4bbed16e94042d9bf01e57eb55...</td>\n",
       "      <td>113</td>\n",
       "      <td>-3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.44</td>\n",
       "      <td>...</td>\n",
       "      <td>[91068, 89184, 49083, 106606, 42996, 95767, 23...</td>\n",
       "      <td>[10, 1, 8, 7, 2, 4, 11, 3, 6, 5, 9]</td>\n",
       "      <td>4-3-3</td>\n",
       "      <td>2018-08-21 18:15:00</td>\n",
       "      <td>Bristol City</td>\n",
       "      <td>{\"Josh Brownhill\": \"DMR\", \"Jack Hunt\": \"DR\", \"...</td>\n",
       "      <td>[172782, 73716, 55563, 110735, 106257, 235530,...</td>\n",
       "      <td>[4, 2, 7, 6, 10, 3, 8, 5, 11, 1, 9]</td>\n",
       "      <td>4-4-2</td>\n",
       "      <td>2018-08-21 18:15:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              gameId  game_optaId  \\\n",
       "0  d0cc49c3230e300b529b270951b3b70b3224481add8354...       991007   \n",
       "1  c0c48eee0b1a42e0d84cb0a947fe2c64f9e1aa7015922f...       990998   \n",
       "2  58b1242154c8055252582229abfc4680460278834c4433...       991001   \n",
       "3  3a604f5616b39eb17fc8d1eed07d5248e387bf400294b2...       991000   \n",
       "\n",
       "             gameDate                                         homeTeamId  \\\n",
       "0 2018-08-21 18:45:00  9ca1f9a87934693b07890de4b4528b0f3ae4065a67ec38...   \n",
       "1 2018-08-21 18:45:00  bc9d5de208258f2f95282c59e9551310be9d319ebc6e4e...   \n",
       "2 2018-08-21 18:45:00  58301066042bbdf19de8fe7d41afc53626b5aa79034712...   \n",
       "3 2018-08-21 18:45:00  e2bfbb5453a7853e049b9434db74d4d06b8c5560ff7cf9...   \n",
       "\n",
       "   homeTeam_optaId                                         awayTeamId  \\\n",
       "0               80  38ca605bcd29a5a37697ca66e533ae817ced71b6bf275c...   \n",
       "1               24  4a625f945d8f58984be0aa7b2ac6409a23ed9cf48e4260...   \n",
       "2               72  bbb63e4ea54b0d60b48a1f8440254d7e656dfbfcbef825...   \n",
       "3               52  d6fe4a4ffbf1e1a0ae9d4bbed16e94042d9bf01e57eb55...   \n",
       "\n",
       "   awayTeam_optaId  tgt_gd  tgt_outcome  preGameOdds1  ...  \\\n",
       "0                2       0          1.0          3.13  ...   \n",
       "1               40       2          0.0          2.05  ...   \n",
       "2               88      -1          2.0          3.09  ...   \n",
       "3              113      -3          2.0          2.44  ...   \n",
       "\n",
       "                                   homeTeamLineupIds  \\\n",
       "0  [40346, 28654, 49539, 169432, 214225, 116215, ...   \n",
       "1  [106423, 12744, 184341, 57714, 103920, 17601, ...   \n",
       "2  [80246, 19152, 193576, 155529, 124120, 41753, ...   \n",
       "3  [91068, 89184, 49083, 106606, 42996, 95767, 23...   \n",
       "\n",
       "                   homeTeamLineupSlots  homeTeamFormation  \\\n",
       "0  [1, 3, 2, 9, 6, 11, 7, 5, 4, 8, 10]            4-2-3-1   \n",
       "1  [4, 9, 8, 3, 10, 1, 2, 5, 6, 11, 7]              4-3-3   \n",
       "2  [7, 6, 2, 1, 4, 3, 11, 8, 5, 9, 10]              4-5-1   \n",
       "3  [10, 1, 8, 7, 2, 4, 11, 3, 6, 5, 9]              4-3-3   \n",
       "\n",
       "   home_team_lineup_received_at  awayTeamName  \\\n",
       "0           2018-08-21 18:15:00  Leeds United   \n",
       "1           2018-08-21 18:15:00  Ipswich Town   \n",
       "2           2018-08-21 18:15:00     Hull City   \n",
       "3           2018-08-21 18:15:00  Bristol City   \n",
       "\n",
       "                                      awayTeamLineup  \\\n",
       "0  {\"Kalvin Phillips\": \"DMC\", \"Jamie Shackleton\":...   \n",
       "1  {\"Jonas Knudsen\": \"DL\", \"Janoi Donacien\": \"DR\"...   \n",
       "2  {\"Evandro Goebel\": \"AMC\", \"Jordy De Wijs\": \"DC...   \n",
       "3  {\"Josh Brownhill\": \"DMR\", \"Jack Hunt\": \"DR\", \"...   \n",
       "\n",
       "                                   awayTeamLineupIds  \\\n",
       "0  [155405, 221610, 98760, 57913, 220037, 38588, ...   \n",
       "1  [82187, 154936, 101881, 115557, 28530, 19910, ...   \n",
       "2  [52287, 173549, 120449, 28541, 107692, 178186,...   \n",
       "3  [172782, 73716, 55563, 110735, 106257, 235530,...   \n",
       "\n",
       "                   awayTeamLineupSlots awayTeamFormation  \\\n",
       "0  [4, 2, 9, 3, 1, 5, 7, 8, 10, 11, 6]           4-1-4-1   \n",
       "1  [3, 2, 10, 7, 1, 8, 9, 11, 4, 5, 6]           4-1-4-1   \n",
       "2  [10, 6, 2, 9, 11, 7, 4, 5, 3, 1, 8]           4-4-1-1   \n",
       "3  [4, 2, 7, 6, 10, 3, 8, 5, 11, 1, 9]             4-4-2   \n",
       "\n",
       "  away_team_lineup_received_at  \n",
       "0          2018-08-21 18:15:00  \n",
       "1          2018-08-21 18:15:00  \n",
       "2          2018-08-21 18:15:00  \n",
       "3          2018-08-21 18:15:00  \n",
       "\n",
       "[4 rows x 27 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fixtures = data_aggregator(\n",
    "    db_hosts=DB_HOSTS, config=CONFIG, db_host=\"prod_atlas\", limit=4\n",
    ")\n",
    "fixtures.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Monkey-patching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "from d3rlpy import torch_utility\n",
    "from d3rlpy.online.buffers import ReplayBuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "torch_utility.torch_api = torch_api\n",
    "ReplayBuffer.append = append\n",
    "ReplayBuffer._add_last_step = add_last_step\n",
    "uncache([\"d3rlpy.torch_utility\",\"d3rlpy.online.buffers\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D3rlpy Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "from betting_agent.Utils.scaler import CustomScaler\n",
    "from betting_agent.Utils.network_architecture import *\n",
    "from d3rlpy.algos import DQN\n",
    "from d3rlpy.online.explorers import LinearDecayEpsilonGreedy\n",
    "from d3rlpy.models.optimizers import OptimizerFactory\n",
    "from d3rlpy.preprocessing.scalers import register_scaler\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We propose a function that will prepare the `Reinforcement learning` algorithm prior to training. Initially, we initialise the `Betting environment` with the supplied data, then we set up the `Scaler`, which will transform our observations to particular features from the Database, and last, we set up the `Buffer`; `D3rlpy` supports both offline and online training tools. In this case, the `Buffer` will try several experiences in order to obtain a useful dataset.\n",
    "\n",
    "Furthemore, we supply additionally an `Optimizer` to update weights and reduce losses for the `Neural Network` and an `Explorer` which will apply the `exploration-exploitation` dilemma which must exist side by side because The majority of the time, the `epsilon-greedy` strategy takes the action with the largest estimated reward. `Exploration` allows us to experiment with new ideas, which are frequently at contradiction with what we have already learned. The procedure starts with 100% `exploration` and subsequently decreases to 10%.\n",
    "\n",
    "We should note that the `D3rlpy` package has several `RL` algorithms; in our situation, we will choose the `DQN` algorithm (Deep Q-Network)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "def rl_algo_preparation(\n",
    "    fixtures: pd.DataFrame,  # All provided games.\n",
    "    algo: d3rlpy.algos = DQN,  # D3rlpy RL algorithm.\n",
    "    algo_batch_size=32,  #  Mini-batch size.\n",
    "    algo_learning_rate=2.5e-4,  # Algo learning rate.\n",
    "    algo_target_update_interval=100,  # Interval to update the target network.\n",
    "    algo_scaler: Scaler = CustomScaler,  # The scaler for data transformation.\n",
    "    optimizer: torch.optim = Adam,  # Algo Optimizer.\n",
    "    optimizer_weight_decay=1e-4,  # Optimizer weight decay.\n",
    "    maxlen_buffer=1000000,  #  The maximum number of data length.\n",
    "    explorer_start_epsilon=1.0,  # The beginning epsilon.\n",
    "    explorer_end_epsilon=0.1,  # The end epsilon.\n",
    "    explorer_duration=100000,  # The scheduling duration.\n",
    "):\n",
    "    \"Prepare RL algorithm components.\"\n",
    "    # Init betting env.\n",
    "    env = BettingEnv(fixtures)\n",
    "\n",
    "    # Init Scaler.\n",
    "    register_scaler(algo_scaler)\n",
    "    custom_scaler = algo_scaler()\n",
    "\n",
    "    # Init Buffer.\n",
    "    buffer = ReplayBuffer(env=env, maxlen=maxlen_buffer)\n",
    "\n",
    "    # Init the epsilon-greedy explorer\n",
    "    explorer = LinearDecayEpsilonGreedy(\n",
    "        start_epsilon=explorer_start_epsilon,\n",
    "        end_epsilon=explorer_end_epsilon,\n",
    "        duration=explorer_duration,\n",
    "    )\n",
    "\n",
    "    # Init Optimizer.\n",
    "    optim_factory = OptimizerFactory(optimizer, weight_decay=optimizer_weight_decay)\n",
    "\n",
    "    # Init RL Algo.\n",
    "    rl_algo = algo(\n",
    "        batch_size=algo_batch_size,\n",
    "        learning_rate=algo_learning_rate,\n",
    "        target_update_interval=algo_target_update_interval,\n",
    "        optim_factory=optim_factory,\n",
    "        scaler=custom_scaler,\n",
    "        encoder_factory=CustomEncoderFactory(feature_size=env.action_space.n),\n",
    "    )\n",
    "\n",
    "    return env, buffer, explorer, rl_algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "from d3rlpy.algos.base import AlgoBase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "AlgoBase.fit_online = fit_online\n",
    "uncache([\"d3rlpy.torch_utility\", \"d3rlpy.online.buffers\", \"d3rlpy.algos.base\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Launch training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def launch_training(\n",
    "    fixtures: pd.DataFrame,  # All provided games.\n",
    "    training_steps: int = 100,  # The number of total steps to train.\n",
    "    n_steps_per_epoch: int = 50,  # The number of steps per epoch.\n",
    "    update_start_step: int = 50,  #  The steps before starting updates.\n",
    "    algo: d3rlpy.algos = DQN,  # D3rlpy RL algorithm.\n",
    "    algo_batch_size=32,  #  Mini-batch size.\n",
    "    algo_learning_rate=2.5e-4,  # Algo learning rate.\n",
    "    algo_target_update_interval=100,  # Interval to update the target network.\n",
    "    algo_scaler: Scaler = CustomScaler,  # The scaler for data transformation.\n",
    "    optimizer: torch.optim = Adam,  # Algo Optimizer.\n",
    "    optimizer_weight_decay=1e-4,  # Optimizer weight decay.\n",
    "    maxlen_buffer=1000000,  #  The maximum number of data length.\n",
    "    explorer_start_epsilon=1.0,  # The beginning epsilon.\n",
    "    explorer_end_epsilon=0.1,  # The end epsilon.\n",
    "    explorer_duration=100,  # The scheduling duration.\n",
    "    show_progress: bool = True,  # Flag to show progress bar for iterations.\n",
    "    save_metrics: bool = True,  # Flag to record metrics. If False, the log directory is not created and the model parameters are not saved.\n",
    "):\n",
    "    \"Launch RL algorithm training.\"\n",
    "    # Get algo params.\n",
    "    env, buffer, explorer, rl_algo = rl_algo_preparation(\n",
    "        fixtures=fixtures,\n",
    "        algo=algo,\n",
    "        algo_batch_size=algo_batch_size,\n",
    "        algo_learning_rate=algo_learning_rate,\n",
    "        algo_target_update_interval=algo_target_update_interval,\n",
    "        algo_scaler=algo_scaler,\n",
    "        optimizer=optimizer,\n",
    "        optimizer_weight_decay=optimizer_weight_decay,\n",
    "        maxlen_buffer=maxlen_buffer,\n",
    "        explorer_start_epsilon=explorer_start_epsilon,\n",
    "        explorer_end_epsilon=explorer_end_epsilon,\n",
    "        explorer_duration=explorer_duration,\n",
    "    )\n",
    "    # Launch training.\n",
    "    rl_algo.fit_online(\n",
    "        env,  # Gym environment.\n",
    "        buffer,  # Buffer.\n",
    "        explorer,  # Explorer.\n",
    "        n_steps=training_steps,  # Train for 'training_steps' steps.\n",
    "        n_steps_per_epoch=n_steps_per_epoch,  # Evaluation is performed every 'n_steps_per_epoch' steps.\n",
    "        update_start_step=update_start_step,  # Parameter update starts after 'update_start_step' steps.\n",
    "        save_metrics=save_metrics,  # Save metrics.\n",
    "        show_progress=show_progress,  # Show progress.\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-06 16:54.38 [info     ] Directory is created at d3rlpy_logs/DQN_online_20230306165438\n",
      "2023-03-06 16:54.38 [debug    ] Fitting scaler...              scler=none\n",
      "2023-03-06 16:54.38 [debug    ] Building model...\n",
      "2023-03-06 16:54.38 [debug    ] Model has been built.\n",
      "2023-03-06 16:54.38 [info     ] Parameters are saved to d3rlpy_logs/DQN_online_20230306165438/params.json params={'action_scaler': None, 'batch_size': 32, 'encoder_factory': {'type': 'custom', 'params': {'feature_size': 16}}, 'gamma': 0.99, 'generated_maxlen': 100000, 'learning_rate': 0.00025, 'n_critics': 1, 'n_frames': 1, 'n_steps': 1, 'optim_factory': {'optim_cls': 'Adam', 'weight_decay': 0.0001}, 'q_func_factory': {'type': 'mean', 'params': {'share_encoder': False}}, 'real_ratio': 1.0, 'reward_scaler': None, 'scaler': {'type': 'none', 'params': {}}, 'target_update_interval': 100, 'use_gpu': None, 'algorithm': 'DQN', 'observation_shape': (30,), 'action_size': 16}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b36da08b0aa648128492149d1392435e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-06 16:55.53 [info     ] Model parameters are saved to d3rlpy_logs/DQN_online_20230306165438/model_100.pt\n",
      "2023-03-06 16:55.53 [info     ] DQN_online_20230306165438: epoch=1 step=100 epoch=1 metrics={'time_inference': 0.36903311729431154, 'time_environment_step': 0.0009215998649597167, 'time_step': 0.741472589969635, 'rollout_return': 2.0720000000000005} step=100\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'ra_teams_ids'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mlaunch_training\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfixtures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfixtures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43malgo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDQN\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43malgo_scaler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCustomScaler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mAdam\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexplorer_duration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_steps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mupdate_start_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     11\u001b[0m \u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36mlaunch_training\u001b[0;34m(fixtures, training_steps, n_steps_per_epoch, update_start_step, algo, algo_batch_size, algo_learning_rate, algo_target_update_interval, algo_scaler, optimizer, optimizer_weight_decay, maxlen_buffer, explorer_start_epsilon, explorer_end_epsilon, explorer_duration, show_progress, save_metrics)\u001b[0m\n\u001b[1;32m     24\u001b[0m env, buffer, explorer, rl_algo \u001b[38;5;241m=\u001b[39m rl_algo_preparation(\n\u001b[1;32m     25\u001b[0m     fixtures\u001b[38;5;241m=\u001b[39mfixtures,\n\u001b[1;32m     26\u001b[0m     algo\u001b[38;5;241m=\u001b[39malgo,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     36\u001b[0m     explorer_duration\u001b[38;5;241m=\u001b[39mexplorer_duration,\n\u001b[1;32m     37\u001b[0m )\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Launch training.\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m \u001b[43mrl_algo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_online\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Gym environment.\u001b[39;49;00m\n\u001b[1;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Buffer.\u001b[39;49;00m\n\u001b[1;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexplorer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Explorer.\u001b[39;49;00m\n\u001b[1;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Train for 'training_steps' steps.\u001b[39;49;00m\n\u001b[1;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_steps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_steps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Evaluation is performed every 'n_steps_per_epoch' steps.\u001b[39;49;00m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43mupdate_start_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mupdate_start_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Parameter update starts after 'update_start_step' steps.\u001b[39;49;00m\n\u001b[1;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_metrics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Save metrics.\u001b[39;49;00m\n\u001b[1;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Show progress.\u001b[39;49;00m\n\u001b[1;32m     48\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Meher/betting_agent/betting_agent/Utils/monkey_patching.py:405\u001b[0m, in \u001b[0;36mfit_online\u001b[0;34m(self, env, buffer, explorer, n_steps, n_steps_per_epoch, update_interval, update_start_step, random_steps, eval_env, eval_epsilon, save_metrics, save_interval, experiment_name, with_timestamp, logdir, verbose, show_progress, tensorboard_dir, timelimit_aware, callback)\u001b[0m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStart training loop of online deep reinforcement learning.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;66;03m# Launch training.\u001b[39;00m\n\u001b[0;32m--> 405\u001b[0m \u001b[43mtrain_single_env\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m    \u001b[49m\u001b[43malgo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m    \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbuffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexplorer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexplorer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_steps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_steps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m    \u001b[49m\u001b[43mupdate_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mupdate_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m    \u001b[49m\u001b[43mupdate_start_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mupdate_start_step\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrandom_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_epsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_epsilon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_metrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    418\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    419\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexperiment_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexperiment_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    420\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwith_timestamp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwith_timestamp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    421\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogdir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogdir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    422\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    423\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    424\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensorboard_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensorboard_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    425\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimelimit_aware\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimelimit_aware\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    426\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    427\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Meher/betting_agent/betting_agent/Utils/monkey_patching.py:343\u001b[0m, in \u001b[0;36mtrain_single_env\u001b[0;34m(algo, env, buffer, explorer, n_steps, n_steps_per_epoch, update_interval, update_start_step, random_steps, eval_env, eval_epsilon, save_metrics, save_interval, experiment_name, with_timestamp, logdir, verbose, show_progress, tensorboard_dir, timelimit_aware, callback)\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;66;03m# Update parameters.\u001b[39;00m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m logger\u001b[38;5;241m.\u001b[39mmeasure_time(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malgorithm_update\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 343\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43malgo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# Record metrics.\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, val \u001b[38;5;129;01min\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m~/anaconda3/envs/matchpred/lib/python3.9/site-packages/d3rlpy/base.py:748\u001b[0m, in \u001b[0;36mLearnableBase.update\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    738\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch: TransitionMiniBatch) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mfloat\u001b[39m]:\n\u001b[1;32m    739\u001b[0m     \u001b[38;5;124;03m\"\"\"Update parameters with mini-batch of data.\u001b[39;00m\n\u001b[1;32m    740\u001b[0m \n\u001b[1;32m    741\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    746\u001b[0m \n\u001b[1;32m    747\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 748\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    749\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_grad_step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/anaconda3/envs/matchpred/lib/python3.9/site-packages/d3rlpy/algos/dqn.py:129\u001b[0m, in \u001b[0;36mDQN._update\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_update\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch: TransitionMiniBatch) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mfloat\u001b[39m]:\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_impl \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, IMPL_NOT_INITIALIZED_ERROR\n\u001b[0;32m--> 129\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_impl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_grad_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_target_update_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    131\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_impl\u001b[38;5;241m.\u001b[39mupdate_target()\n",
      "File \u001b[0;32m~/anaconda3/envs/matchpred/lib/python3.9/site-packages/d3rlpy/torch_utility.py:313\u001b[0m, in \u001b[0;36mtrain_api.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;28mself\u001b[39m: Any, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[1;32m    312\u001b[0m     set_train_mode(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 313\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Meher/betting_agent/betting_agent/Utils/monkey_patching.py:69\u001b[0m, in \u001b[0;36mtorch_api.<locals>._torch_api.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     tensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(val, TransitionMiniBatch):\n\u001b[0;32m---> 69\u001b[0m     tensor \u001b[38;5;241m=\u001b[39m \u001b[43mTorchMiniBatch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m        \u001b[49m\u001b[43mval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m        \u001b[49m\u001b[43maction_scaler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction_scaler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreward_scaler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreward_scaler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     77\u001b[0m     tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\n\u001b[1;32m     78\u001b[0m         data\u001b[38;5;241m=\u001b[39mval,\n\u001b[1;32m     79\u001b[0m         dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32,\n\u001b[1;32m     80\u001b[0m         device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice,\n\u001b[1;32m     81\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/matchpred/lib/python3.9/site-packages/d3rlpy/torch_utility.py:181\u001b[0m, in \u001b[0;36mTorchMiniBatch.__init__\u001b[0;34m(self, batch, device, scaler, action_scaler, reward_scaler)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;66;03m# apply scaler\u001b[39;00m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m scaler:\n\u001b[0;32m--> 181\u001b[0m     observations \u001b[38;5;241m=\u001b[39m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservations\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m     next_observations \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mtransform(next_observations)\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m action_scaler:\n",
      "File \u001b[0;32m~/Meher/betting_agent/betting_agent/Utils/scaler.py:36\u001b[0m, in \u001b[0;36mCustomScaler.transform\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReturns processed observation.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Real-Analytics Home Team Id.\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m ra_home_team_id \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mra_teams_ids\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Real-Analytics Away Team Id.\u001b[39;00m\n\u001b[1;32m     39\u001b[0m ra_away_team_id \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mra_teams_ids[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'ra_teams_ids'"
     ]
    }
   ],
   "source": [
    "launch_training(\n",
    "    fixtures=fixtures,\n",
    "    algo=DQN,\n",
    "    algo_scaler=CustomScaler,\n",
    "    optimizer=Adam,\n",
    "    explorer_duration=1000,\n",
    "    training_steps=1000,\n",
    "    n_steps_per_epoch=100,  \n",
    "    update_start_step=100,\n",
    "    save_metrics=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev import nbdev_export\n",
    "\n",
    "nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
