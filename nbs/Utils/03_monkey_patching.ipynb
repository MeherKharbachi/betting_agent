{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cc4a15-eae6-47f7-8903-7fb393b4a171",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp Utils.monkey_patching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae97ec29-45a6-4350-8cc8-7eb204b5cf2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from IPython.core.debugger import set_trace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b8623a-1be8-4ca6-a2f7-f92438b24bcf",
   "metadata": {},
   "source": [
    "# Monkey Patching\n",
    "\n",
    "> Modify D3RLPY package behaviour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e379899c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import gym\n",
    "from betting_env.betting_env import Observation\n",
    "from inspect import signature\n",
    "from d3rlpy.torch_utility import _WithDeviceAndScalerProtocol, TorchMiniBatch\n",
    "from d3rlpy.dataset import TransitionMiniBatch, Transition\n",
    "from d3rlpy.online.iterators import AlgoProtocol\n",
    "from d3rlpy.online.explorers import Explorer\n",
    "from d3rlpy.algos.base import _assert_action_space\n",
    "from d3rlpy.logger import D3RLPyLogger\n",
    "from d3rlpy.preprocessing.stack import StackedObservation\n",
    "from d3rlpy.online.iterators import _setup_algo\n",
    "from d3rlpy.metrics.scorer import evaluate_on_environment\n",
    "from d3rlpy.online.buffers import ReplayBuffer\n",
    "from typing import Any, Callable, Dict, List, Optional, Union\n",
    "from torch.utils.data._utils.collate import default_collate\n",
    "from tqdm.auto import trange"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6bbcf4",
   "metadata": {},
   "source": [
    "## Functions\n",
    "\n",
    "In this package, we will employ a technique known as `Monkey-patching` which tries to modify and improve the behavior of third-party libraries (in our case, `D3RLPY`).\n",
    "\n",
    "In this regard, we will modify certain functions supplied in the d3rlpy package in order to adjust our observation output defined in the `betting_env` package."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf32d23",
   "metadata": {},
   "source": [
    "### torch_api function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ff3358",
   "metadata": {},
   "source": [
    "This function is included in the d3rlpy `torch_utility.py` file and is designed to send observations,rewards and actions to the appropriate `Scaler` based on their datatype. As a result, we will add our `Observation class` type developed in the `betting_env` package to these criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25d74ac-d7eb-4ec8-bc1d-b6492d2487c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def torch_api(\n",
    "    scaler_targets: Optional[\n",
    "        List[str]\n",
    "    ] = None,  # On which objects the scaler will be applied.\n",
    "    action_scaler_targets: Optional[\n",
    "        List[str]\n",
    "    ] = None,  # On which objects the action scaler will be applied.\n",
    "    reward_scaler_targets: Optional[\n",
    "        List[str]\n",
    "    ] = None,  # On which objects the reward scaler will be applied.\n",
    ") -> Callable[..., np.ndarray]:\n",
    "    \"Dispatch inputs to provided Scalers (Observations, Actions and Rewards)\"\n",
    "\n",
    "    def _torch_api(f: Callable[..., np.ndarray]) -> Callable[..., np.ndarray]:\n",
    "        # Get argument names.\n",
    "        sig = signature(f)\n",
    "        arg_keys = list(sig.parameters.keys())[1:]\n",
    "\n",
    "        def wrapper(\n",
    "            self: _WithDeviceAndScalerProtocol, *args: Any, **kwargs: Any\n",
    "        ) -> np.ndarray:\n",
    "            tensors: List[Union[torch.Tensor, TorchMiniBatch]] = []\n",
    "            # Convert all args to torch.Tensor.\n",
    "            for i, val in enumerate(args):\n",
    "                tensor: Union[torch.Tensor, TorchMiniBatch]\n",
    "                if isinstance(val, torch.Tensor) or isinstance(val, Observation):\n",
    "                    tensor = val\n",
    "                elif isinstance(val, list):\n",
    "                    val = list(self.scaler.transform(v) for v in val)\n",
    "                    tensor = default_collate(val)\n",
    "                    tensor = tensor[0].to(self.device).float()\n",
    "\n",
    "                elif isinstance(val, np.ndarray):\n",
    "                    if val.dtype == np.uint8:\n",
    "                        dtype = torch.uint8\n",
    "                    else:\n",
    "                        dtype = torch.float32\n",
    "                    tensor = torch.tensor(\n",
    "                        data=val.numerical_observation,\n",
    "                        dtype=dtype,\n",
    "                        device=self.device,\n",
    "                    )\n",
    "                elif val is None:\n",
    "                    tensor = None\n",
    "                elif isinstance(val, TransitionMiniBatch):\n",
    "                    tensor = TorchMiniBatch(\n",
    "                        val,\n",
    "                        self.device,\n",
    "                        scaler=self.scaler,\n",
    "                        action_scaler=self.action_scaler,\n",
    "                        reward_scaler=self.reward_scaler,\n",
    "                    )\n",
    "                else:\n",
    "                    tensor = torch.tensor(\n",
    "                        data=val,\n",
    "                        dtype=torch.float32,\n",
    "                        device=self.device,\n",
    "                    )\n",
    "\n",
    "                if isinstance(val, Observation):\n",
    "                    # Preprocess.\n",
    "                    if self.scaler and scaler_targets:\n",
    "                        if arg_keys[i] in scaler_targets:\n",
    "                            tensor = self.scaler.transform(tensor)\n",
    "                            # Convert to tensor.\n",
    "                            tensor = torch.tensor(data=tensor, device=self.device)\n",
    "\n",
    "                    # Preprocess action.\n",
    "                    if self.action_scaler and action_scaler_targets:\n",
    "                        if arg_keys[i] in action_scaler_targets:\n",
    "                            tensor = self.action_scaler.transform(tensor)\n",
    "\n",
    "                    # Preprocessing reward.\n",
    "                    if self.reward_scaler and reward_scaler_targets:\n",
    "                        if arg_keys[i] in reward_scaler_targets:\n",
    "                            tensor = self.reward_scaler.transform(tensor)\n",
    "\n",
    "                    # Make sure if the tensor is float32 type.\n",
    "                    if tensor is not None and tensor.dtype != torch.float32:\n",
    "                        tensor = tensor.float()\n",
    "\n",
    "                tensors.append(tensor)\n",
    "            return f(self, *tensors, **kwargs)\n",
    "\n",
    "        return wrapper\n",
    "\n",
    "    return _torch_api"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6812fb87",
   "metadata": {},
   "source": [
    "### ReplayBuffer functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91caa01",
   "metadata": {},
   "source": [
    "both functions,`append` and `add_last_step` included in the d3rlpy `buffers.py` file, are used to gather `Agent` experiences and interactions with the `Betting environment` in order to train the model. As a result, we will adapt this alteration because our Observation is no longer the same after the transformation step since it has a different shape ( Adding new features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83dbb252",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "def append(\n",
    "    self,\n",
    "    observation: np.ndarray,  # The processed observation, shape = (1, new_shape), new_shape is the number of features extracted from the database in the 'Scaler' call\n",
    "    action: np.ndarray,  # Chosen action.\n",
    "    reward: float,  # Reward from the chosen action.\n",
    "    terminal: float,  # Terminal (done) flag.\n",
    "    clip_episode: Optional[\n",
    "        bool\n",
    "    ] = None,  # Flag to clip the current episode. If ``None``, th episode is clipped based on ``terminal``.\n",
    ") -> None:\n",
    "    \"Append observation, action, reward and terminal flag to buffer\"\n",
    "\n",
    "    # If None, use terminal.\n",
    "    if clip_episode is None:\n",
    "        clip_episode = bool(terminal)\n",
    "\n",
    "    # Validation.\n",
    "    if isinstance(action, np.ndarray):\n",
    "        assert action.shape[0] == self._action_size\n",
    "    else:\n",
    "        action = int(action)\n",
    "        assert action < self._action_size\n",
    "    # Not allow terminal=True and clip_episode=False\n",
    "    assert not (terminal and not clip_episode)\n",
    "\n",
    "    # Create Transition object.\n",
    "    if self._prev_observation is not None:\n",
    "        if isinstance(terminal, bool):\n",
    "            terminal = 1.0 if terminal else 0.0\n",
    "\n",
    "        transition = Transition(\n",
    "            observation_shape=(observation.shape[1],),  # Observation shape.\n",
    "            action_size=self._action_size,  # Number of actions.\n",
    "            observation=self._prev_observation,  # Current observation.\n",
    "            action=self._prev_action,  # Chosen action.\n",
    "            reward=self._prev_reward,  # Profit from the action.\n",
    "            next_observation=observation,  # Next observation.\n",
    "            terminal=terminal,  # Done flag.\n",
    "            prev_transition=self._prev_transition,  # Previous transition.\n",
    "        )\n",
    "        if self._prev_transition:\n",
    "            self._prev_transition.next_transition = transition\n",
    "\n",
    "        self._transitions.append(transition)\n",
    "        self._prev_transition = transition\n",
    "\n",
    "    self._prev_observation = observation\n",
    "    self._prev_action = action\n",
    "    self._prev_reward = reward\n",
    "    self._prev_terminal = terminal\n",
    "\n",
    "    if clip_episode:\n",
    "        # skip the timeout state\n",
    "        if terminal:\n",
    "            # add the terminal state\n",
    "            self._add_last_step(observation)\n",
    "        self.clip_episode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16578920",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "def add_last_step(\n",
    "    self,\n",
    "    observation: np.ndarray,  # The last processed observation, shape = (1, new_shape), new_shape is the number of features extracted from the database in the 'Scaler' call\n",
    ") -> None:\n",
    "    \"Add the observation of the last step.\"\n",
    "    # Validation.\n",
    "    assert self._prev_terminal\n",
    "    assert self._prev_observation is not None\n",
    "    # Create Transition object.\n",
    "    transition = Transition(\n",
    "        observation_shape=(observation.shape[1],),  # Observation shape.\n",
    "        action_size=self._action_size,  # Number of actions.\n",
    "        observation=self._prev_observation,  # Current observation.\n",
    "        action=self._prev_action,  # Chosen action.\n",
    "        reward=self._prev_reward,  # Profit from the action.\n",
    "        next_observation=np.zeros_like(self._prev_observation),  # Next observation.\n",
    "        terminal=1.0,  # Done flag.\n",
    "        prev_transition=self._prev_transition,  # Previous transition.\n",
    "    )\n",
    "    if self._prev_transition:\n",
    "        self._prev_transition.next_transition = transition\n",
    "    self._transitions.append(transition)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbc0682",
   "metadata": {},
   "source": [
    "## Iterators"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cdc686e",
   "metadata": {},
   "source": [
    "This function, which is included in the D3rlpy `iterators.py` file, is intended to start an Online `Deep Reinforcement Learning` training loop. This function takes as input the desired `RL algorithm`, the `environment` in which the `Agent` will learn, a `buffer` in which all agent interactions are recorded, the `explorer`, and the other parameters indicated below.\n",
    "\n",
    "We used monkey patching in this function to adapt it to our observation datastructure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298443be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "def train_single_env(\n",
    "    algo: AlgoProtocol,  # Algorithm object.\n",
    "    env: gym.Env,  # Gym-like environment.\n",
    "    buffer: ReplayBuffer,  # Replay buffer.\n",
    "    explorer: Optional[Explorer] = None,  # Action explorer.\n",
    "    n_steps: int = 1000000,  # The number of total steps to train.\n",
    "    n_steps_per_epoch: int = 10000,  # The number of steps per epoch.\n",
    "    update_interval: int = 1,  # The number of steps per update.\n",
    "    update_start_step: int = 0,  # The steps before starting updates.\n",
    "    random_steps: int = 0,  # The steps for the initial random explortion.\n",
    "    eval_env: Optional[\n",
    "        gym.Env\n",
    "    ] = None,  # Gym-like environment. If None, evaluation is skipped.\n",
    "    eval_epsilon: float = 0.0,  # Epsilon`-greedy factor during evaluation.\n",
    "    save_metrics: bool = True,  #  Flag to record metrics. If False, the log directory is not created and the model parameters are not saved.\n",
    "    save_interval: int = 1,  # The number of epochs before saving models.\n",
    "    experiment_name: Optional[\n",
    "        str\n",
    "    ] = None,  # Experiment name for logging. If not passed, the directory name will be ``{class name}_online_{timestamp}``.\n",
    "    with_timestamp: bool = True,  # Flag to add timestamp string to the last of directory name.\n",
    "    logdir: str = \"d3rlpy_logs\",  # Root directory name to save logs.\n",
    "    verbose: bool = True,  # Flag to show logged information on stdout.\n",
    "    show_progress: bool = True,  # Flag to show progress bar for iterations.\n",
    "    tensorboard_dir: Optional[\n",
    "        str\n",
    "    ] = None,  # Directory to save logged information in tensorboard (additional to the csv data).  if ``None``, thedirectory will not be created.\n",
    "    timelimit_aware: bool = True,  # Flag to turn ``terminal`` flag ``False`` when ``TimeLimit.truncated`` flag is ``True``, which is designed to incorporate with ``gym.wrappers.TimeLimit``.\n",
    "    callback: Optional[\n",
    "        Callable[[AlgoProtocol, int, int], None]\n",
    "    ] = None,  # Callable function that takes ``(algo, epoch, total_step)``, which is called at the end of epochs.\n",
    ") -> None:\n",
    "    \"Start training loop of online deep reinforcement learning.\"\n",
    "\n",
    "    # setup logger.\n",
    "    if experiment_name is None:\n",
    "        experiment_name = algo.__class__.__name__ + \"_online\"\n",
    "\n",
    "    logger = D3RLPyLogger(\n",
    "        experiment_name,\n",
    "        save_metrics=save_metrics,\n",
    "        root_dir=logdir,\n",
    "        verbose=verbose,\n",
    "        tensorboard_dir=tensorboard_dir,\n",
    "        with_timestamp=with_timestamp,\n",
    "    )\n",
    "    algo.set_active_logger(logger)\n",
    "\n",
    "    # Initialize algorithm parameters.\n",
    "    _setup_algo(algo, env)\n",
    "\n",
    "    # Test observation shape.(Images or not)\n",
    "    observation_shape = env.observation_space.shape\n",
    "    is_image = len(observation_shape) == 3\n",
    "\n",
    "    # Prepare stacked observation.\n",
    "    if is_image:\n",
    "        stacked_frame = StackedObservation(observation_shape, algo.n_frames)\n",
    "\n",
    "    # Save hyperparameters.\n",
    "    algo.save_params(logger)\n",
    "\n",
    "    # Switch based on show_progress flag.\n",
    "    xrange = trange if show_progress else range\n",
    "\n",
    "    # Setup evaluation scorer\n",
    "    eval_scorer: Optional[Callable[..., float]]\n",
    "    if eval_env:\n",
    "        eval_scorer = evaluate_on_environment(eval_env, epsilon=eval_epsilon)\n",
    "    else:\n",
    "        eval_scorer = None\n",
    "\n",
    "    # Start training loop.\n",
    "    observation = env.reset()\n",
    "    rollout_return = 0.0\n",
    "    for total_step in xrange(1, n_steps + 1):\n",
    "        with logger.measure_time(\"step\"):\n",
    "            # Stack observation if necessary (Image case)\n",
    "            if is_image:\n",
    "                stacked_frame.append(observation)\n",
    "                fed_observation = stacked_frame.eval()\n",
    "            else:\n",
    "                observation = observation.astype(\"f4\")\n",
    "                fed_observation = observation\n",
    "\n",
    "            # Sample exploration action.\n",
    "            with logger.measure_time(\"inference\"):\n",
    "                # Case random exploration.\n",
    "                if total_step < random_steps:\n",
    "                    action = env.action_space.sample()\n",
    "                # Case specified Explorer.\n",
    "                elif explorer:\n",
    "                    x = fed_observation.reshape((1,) + fed_observation.shape)\n",
    "                    action = explorer.sample(algo, x, total_step)[0]\n",
    "                else:\n",
    "                    action = algo.sample_action([fed_observation])[0]\n",
    "            # Step environment.\n",
    "            with logger.measure_time(\"environment_step\"):\n",
    "                # Return observation, reward , done and info flags.\n",
    "                next_observation, reward, terminal, info = env.step(action)\n",
    "                # Add reward for the current step\n",
    "                rollout_return += reward\n",
    "\n",
    "            # Special case for TimeLimit wrapper\n",
    "            if timelimit_aware and \"TimeLimit.truncated\" in info:\n",
    "                clip_episode = True\n",
    "                terminal = False\n",
    "            else:\n",
    "                clip_episode = terminal\n",
    "\n",
    "            # Apply transformation on observation before buffer storage.\n",
    "            if algo.scaler:\n",
    "                observation = algo.scaler.transform(observation)\n",
    "\n",
    "            # Store observation.\n",
    "            buffer.append(\n",
    "                observation=observation,\n",
    "                action=action,\n",
    "                reward=reward,\n",
    "                terminal=terminal,\n",
    "                clip_episode=clip_episode,\n",
    "            )\n",
    "            # Reset if terminated or pass to the next observation.\n",
    "            if clip_episode:\n",
    "                observation = env.reset()\n",
    "                logger.add_metric(\"rollout_return\", rollout_return)\n",
    "                rollout_return = 0.0\n",
    "                # for image observation\n",
    "                if is_image:\n",
    "                    stacked_frame.clear()\n",
    "            else:\n",
    "                observation = next_observation\n",
    "\n",
    "            # Psuedo epoch count.\n",
    "            epoch = total_step // n_steps_per_epoch\n",
    "            if total_step > update_start_step and len(buffer) > algo.batch_size:\n",
    "                if total_step % update_interval == 0:\n",
    "                    # Sample mini-batch.\n",
    "                    with logger.measure_time(\"sample_batch\"):\n",
    "                        batch = buffer.sample(\n",
    "                            batch_size=algo.batch_size,\n",
    "                            n_frames=algo.n_frames,\n",
    "                            n_steps=algo.n_steps,\n",
    "                            gamma=algo.gamma,\n",
    "                        )\n",
    "\n",
    "                    # Update parameters.\n",
    "                    with logger.measure_time(\"algorithm_update\"):\n",
    "                        loss = algo.update(batch)\n",
    "\n",
    "                    # Record metrics.\n",
    "                    for name, val in loss.items():\n",
    "                        logger.add_metric(name, val)\n",
    "\n",
    "            # Call callback if given.\n",
    "            if callback:\n",
    "                callback(algo, epoch, total_step)\n",
    "\n",
    "        if epoch > 0 and total_step % n_steps_per_epoch == 0:\n",
    "            # Evaluation.\n",
    "            if eval_scorer:\n",
    "                logger.add_metric(\"evaluation\", eval_scorer(algo))\n",
    "\n",
    "            if epoch % save_interval == 0:\n",
    "                logger.save_model(total_step, algo)\n",
    "\n",
    "            # Save metrics.\n",
    "            logger.commit(epoch, total_step)\n",
    "\n",
    "    # Clip the last episode.\n",
    "    buffer.clip_episode()\n",
    "\n",
    "    # Close logger.\n",
    "    logger.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38275871",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "def fit_online(\n",
    "    self,\n",
    "    env: gym.Env,  # Gym-like environment.\n",
    "    buffer: ReplayBuffer,  # Replay buffer.\n",
    "    explorer: Optional[Explorer] = None,  # Action explorer.\n",
    "    n_steps: int = 1000000,  # The number of total steps to train.\n",
    "    n_steps_per_epoch: int = 10000,  # The number of steps per epoch.\n",
    "    update_interval: int = 1,  # The number of steps per update.\n",
    "    update_start_step: int = 0,  #  The steps before starting updates.\n",
    "    random_steps: int = 0,  # The steps for the initial random explortion.\n",
    "    eval_env: Optional[\n",
    "        gym.Env\n",
    "    ] = None,  # Gym-like environment. If None, evaluation is skipped.\n",
    "    eval_epsilon: float = 0.0,  # Epsilon-greedy factor during evaluation.\n",
    "    save_metrics: bool = True,  # Flag to record metrics. If False, the log directory is not created and the model parameters are not saved.\n",
    "    save_interval: int = 1,  #  The number of epochs before saving models.\n",
    "    experiment_name: Optional[\n",
    "        str\n",
    "    ] = None,  # Experiment name for logging. If not passed, the directory name will be ``{class name}_online_{timestamp}``.\n",
    "    with_timestamp: bool = True,  # Flag to add timestamp string to the last of directory name.\n",
    "    logdir: str = \"d3rlpy_logs\",  # Root directory name to save logs.\n",
    "    verbose: bool = True,  # Flag to show logged information on stdout.\n",
    "    show_progress: bool = True,  # Flag to show progress bar for iterations.\n",
    "    tensorboard_dir: Optional[\n",
    "        str\n",
    "    ] = None,  # Directory to save logged information in tensorboard (additional to the csv data). If ``None``, the directory will not be created.\n",
    "    timelimit_aware: bool = True,  # Flag to turn ``terminal`` flag ``False`` when``TimeLimit.truncated`` flag is ``True``, which is designed to incorporate with ``gym.wrappers.TimeLimit``.\n",
    "    callback: Optional[\n",
    "        Callable[[AlgoProtocol, int, int], None]\n",
    "    ] = None,  # Callable function that takes ``(algo, epoch, total_step)``, which is called at the end of epochs.\n",
    ") -> None:\n",
    "    \"Start training loop of online deep reinforcement learning.\"\n",
    "\n",
    "    # Launch training.\n",
    "    train_single_env(\n",
    "        algo=self,\n",
    "        env=env,\n",
    "        buffer=buffer,\n",
    "        explorer=explorer,\n",
    "        n_steps=n_steps,\n",
    "        n_steps_per_epoch=n_steps_per_epoch,\n",
    "        update_interval=update_interval,\n",
    "        update_start_step=update_start_step,\n",
    "        random_steps=random_steps,\n",
    "        eval_env=eval_env,\n",
    "        eval_epsilon=eval_epsilon,\n",
    "        save_metrics=save_metrics,\n",
    "        save_interval=save_interval,\n",
    "        experiment_name=experiment_name,\n",
    "        with_timestamp=with_timestamp,\n",
    "        logdir=logdir,\n",
    "        verbose=verbose,\n",
    "        show_progress=show_progress,\n",
    "        tensorboard_dir=tensorboard_dir,\n",
    "        timelimit_aware=timelimit_aware,\n",
    "        callback=callback,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8594c29-a9fc-48fc-9a44-95126079ddf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev import nbdev_export\n",
    "\n",
    "nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
