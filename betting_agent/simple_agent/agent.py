# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/simpleAgent/03_simple_agent.ipynb.

# %% auto 0
__all__ = ['rl_algo_preparation', 'launch_training']

# %% ../../nbs/simpleAgent/03_simple_agent.ipynb 3
import d3rlpy
import pandas as pd
import torch
from betting_env.betting_env import BettingEnv
from betting_env.utils.data_extractor import *
from torch.optim import Adam

from ..config.localconfig import CONFIG, DB_HOSTS
from ..patching.monkey_patching import *
from ..patching.uncache import *

# %% ../../nbs/simpleAgent/03_simple_agent.ipynb 6
from d3rlpy import torch_utility
from d3rlpy.online.buffers import ReplayBuffer

# %% ../../nbs/simpleAgent/03_simple_agent.ipynb 7
torch_utility.torch_api = torch_api
ReplayBuffer.append = append
ReplayBuffer._add_last_step = add_last_step
uncache(["d3rlpy.torch_utility", "d3rlpy.online.buffers"])

# %% ../../nbs/simpleAgent/03_simple_agent.ipynb 9
from d3rlpy.algos import DQN
from d3rlpy.algos.base import AlgoBase
from d3rlpy.models.optimizers import OptimizerFactory
from d3rlpy.online.explorers import LinearDecayEpsilonGreedy
from d3rlpy.preprocessing.scalers import Scaler

AlgoBase.fit_online = fit_online

from .network_architecture import *
from .scaler import SimpleScaler

uncache(["d3rlpy.torch_utility", "d3rlpy.online.buffers", "d3rlpy.algos.base", "d3rlpy.preprocessing.scalers"])

# %% ../../nbs/simpleAgent/03_simple_agent.ipynb 11
def rl_algo_preparation(
    fixtures: pd.DataFrame,  # All provided games.
    algo: d3rlpy.algos,  # D3rlpy RL algorithm.
    algo_batch_size=32,  #  Mini-batch size.
    algo_learning_rate=2.5e-4,  # Algo learning rate.
    algo_target_update_interval=100,  # Interval to update the target network.
    algo_scaler: str = "simple", # name of the scaler to apply
    optimizer: torch.optim = Adam,  # Algo Optimizer.
    optimizer_weight_decay=1e-4,  # Optimizer weight decay.
    maxlen_buffer=1000000,  #  The maximum number of data length.
    explorer_start_epsilon=1.0,  # The beginning epsilon.
    explorer_end_epsilon=0.1,  # The end epsilon.
    explorer_duration=100000,  # The scheduling duration.
    **kwargs,  # extra arguments accepted by `SimpleEncoderFactory()`
):
    "Prepare RL algorithm components."
    # Init betting env.
    env = BettingEnv(fixtures)
    
    # scaler
    scaler = SimpleScaler()

    # Init Buffer.
    buffer = ReplayBuffer(env=env, maxlen=maxlen_buffer)

    # Init the epsilon-greedy explorer
    explorer = LinearDecayEpsilonGreedy(
        start_epsilon=explorer_start_epsilon,
        end_epsilon=explorer_end_epsilon,
        duration=explorer_duration,
    )

    # Init Optimizer.
    optim_factory = OptimizerFactory(optimizer, weight_decay=optimizer_weight_decay)

    # Init RL Algo.
    rl_algo = algo(
        batch_size=algo_batch_size,
        learning_rate=algo_learning_rate,
        target_update_interval=algo_target_update_interval,
        optim_factory=optim_factory,
        scaler=scaler,
        encoder_factory=SimpleEncoderFactory(
            feature_size=env.action_space.n,
            observation_size=scaler.OBS,
            **kwargs,
        ),
    )

    return env, buffer, explorer, rl_algo

# %% ../../nbs/simpleAgent/03_simple_agent.ipynb 12
def launch_training(
    fixtures: pd.DataFrame,  # All provided games.
    algo: d3rlpy.algos,  # D3rlpy RL algorithm.
    training_steps: int = 100,  # The number of total steps to train.
    n_steps_per_epoch: int = 50,  # The number of steps per epoch.
    update_start_step: int = 50,  #  The steps before starting updates.
    algo_batch_size: int = 32,  #  Mini-batch size.
    algo_learning_rate: float = 2.5e-4,  # Algo learning rate.
    algo_target_update_interval: int = 100,  # Interval to update the target network.
    algo_scaler: Scaler = SimpleScaler,  # The scaler for data transformation.
    optimizer: torch.optim = Adam,  # Algo Optimizer.
    optimizer_weight_decay: float = 1e-4,  # Optimizer weight decay.
    maxlen_buffer: int = 1000000,  #  The maximum number of data length.
    explorer_start_epsilon: float = 1.0,  # The beginning epsilon.
    explorer_end_epsilon: float = 0.1,  # The end epsilon.
    explorer_duration: int = 100,  # The scheduling duration.
    eval_epsilon: float = 0.3,  # Greedy-epsilon for evaluation.
    show_progress: bool = True,  # Flag to show progress bar for iterations.
    save_metrics: bool = True,  # Flag to record metrics. If False, the log directory is not created and the model parameters are not saved.
):
    "Launch RL algorithm training."
    # Get algo params.
    env, buffer, explorer, rl_algo = rl_algo_preparation(
        fixtures=fixtures,
        algo=algo,
        algo_batch_size=algo_batch_size,
        algo_learning_rate=algo_learning_rate,
        algo_target_update_interval=algo_target_update_interval,
        algo_scaler=algo_scaler,
        optimizer=optimizer,
        optimizer_weight_decay=optimizer_weight_decay,
        maxlen_buffer=maxlen_buffer,
        explorer_start_epsilon=explorer_start_epsilon,
        explorer_end_epsilon=explorer_end_epsilon,
        explorer_duration=explorer_duration,
    )
    # Launch training.
    eval_env = BettingEnv(fixtures)
    rl_algo.fit_online(
        env,  # Gym environment.
        buffer,  # Buffer.
        explorer,  # Explorer.
        n_steps=training_steps,  # Train for 'training_steps' steps.
        n_steps_per_epoch=n_steps_per_epoch,  # Evaluation is performed every 'n_steps_per_epoch' steps.
        update_start_step=update_start_step,  # Parameter update starts after 'update_start_step' steps.
        save_metrics=save_metrics,  # Save metrics.
        show_progress=show_progress,  # Show progress.
        eval_env=eval_env,  # Environment for evaluation.
        eval_epsilon=eval_epsilon,  # Greedy-epsilon for evaluation.
    )
