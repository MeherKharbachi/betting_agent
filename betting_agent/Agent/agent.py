# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/Agent/00_agent.ipynb.

# %% auto 0
__all__ = ['rl_algo_preparation', 'launch_training']

# %% ../../nbs/Agent/00_agent.ipynb 3
import pandas as pd
import d3rlpy
import torch
from ..Utils.uncache import *
from ..Utils.monkey_patching import *
from d3rlpy.preprocessing.scalers import Scaler
from ..config.localconfig import CONFIG, DB_HOSTS
from betting_env.betting_env import BettingEnv
from betting_env.utils.data_extractor import *

# %% ../../nbs/Agent/00_agent.ipynb 7
from d3rlpy import torch_utility
from d3rlpy.online.buffers import ReplayBuffer

# %% ../../nbs/Agent/00_agent.ipynb 8
torch_utility.torch_api = torch_api
ReplayBuffer.append = append
ReplayBuffer._add_last_step = add_last_step
uncache(["d3rlpy.torch_utility","d3rlpy.online.buffers"])

# %% ../../nbs/Agent/00_agent.ipynb 10
from ..Utils.scaler import CustomScaler
from ..Utils.network_architecture import *
from d3rlpy.algos import DQN
from d3rlpy.online.explorers import LinearDecayEpsilonGreedy
from d3rlpy.models.optimizers import OptimizerFactory
from d3rlpy.preprocessing.scalers import register_scaler
from torch.optim import Adam

# %% ../../nbs/Agent/00_agent.ipynb 12
def rl_algo_preparation(
    fixtures: pd.DataFrame,  # All provided games.
    algo: d3rlpy.algos = DQN,  # D3rlpy RL algorithm.
    algo_batch_size=32,  #  Mini-batch size.
    algo_learning_rate=2.5e-4,  # Algo learning rate.
    algo_target_update_interval=100,  # Interval to update the target network.
    algo_scaler: Scaler = CustomScaler,  # The scaler for data transformation.
    optimizer: torch.optim = Adam,  # Algo Optimizer.
    optimizer_weight_decay=1e-4,  # Optimizer weight decay.
    maxlen_buffer=1000000,  #  The maximum number of data length.
    explorer_start_epsilon=1.0,  # The beginning epsilon.
    explorer_end_epsilon=0.1,  # The end epsilon.
    explorer_duration=100000,  # The scheduling duration.
):
    "Prepare RL algorithm components."
    # Init betting env.
    env = BettingEnv(fixtures)

    # Init Scaler.
    register_scaler(algo_scaler)
    custom_scaler = algo_scaler()

    # Init Buffer.
    buffer = ReplayBuffer(env=env, maxlen=maxlen_buffer)

    # Init the epsilon-greedy explorer
    explorer = LinearDecayEpsilonGreedy(
        start_epsilon=explorer_start_epsilon,
        end_epsilon=explorer_end_epsilon,
        duration=explorer_duration,
    )

    # Init Optimizer.
    optim_factory = OptimizerFactory(optimizer, weight_decay=optimizer_weight_decay)

    # Init RL Algo.
    rl_algo = algo(
        batch_size=algo_batch_size,
        learning_rate=algo_learning_rate,
        target_update_interval=algo_target_update_interval,
        optim_factory=optim_factory,
        scaler=custom_scaler,
        encoder_factory=CustomEncoderFactory(feature_size=env.action_space.n),
    )

    return env, buffer, explorer, rl_algo

# %% ../../nbs/Agent/00_agent.ipynb 13
from d3rlpy.algos.base import AlgoBase

# %% ../../nbs/Agent/00_agent.ipynb 14
AlgoBase.fit_online = fit_online
uncache(["d3rlpy.torch_utility", "d3rlpy.online.buffers", "d3rlpy.algos.base"])

# %% ../../nbs/Agent/00_agent.ipynb 16
def launch_training(
    fixtures: pd.DataFrame,  # All provided games.
    training_steps: int = 100,  # The number of total steps to train.
    n_steps_per_epoch: int = 50,  # The number of steps per epoch.
    update_start_step: int = 50,  #  The steps before starting updates.
    algo: d3rlpy.algos = DQN,  # D3rlpy RL algorithm.
    algo_batch_size=32,  #  Mini-batch size.
    algo_learning_rate=2.5e-4,  # Algo learning rate.
    algo_target_update_interval=100,  # Interval to update the target network.
    algo_scaler: Scaler = CustomScaler,  # The scaler for data transformation.
    optimizer: torch.optim = Adam,  # Algo Optimizer.
    optimizer_weight_decay=1e-4,  # Optimizer weight decay.
    maxlen_buffer=1000000,  #  The maximum number of data length.
    explorer_start_epsilon=1.0,  # The beginning epsilon.
    explorer_end_epsilon=0.1,  # The end epsilon.
    explorer_duration=100,  # The scheduling duration.
    show_progress: bool = True,  # Flag to show progress bar for iterations.
    save_metrics: bool = True,  # Flag to record metrics. If False, the log directory is not created and the model parameters are not saved.
):
    "Launch RL algorithm training."
    # Get algo params.
    env, buffer, explorer, rl_algo = rl_algo_preparation(
        fixtures=fixtures,
        algo=algo,
        algo_batch_size=algo_batch_size,
        algo_learning_rate=algo_learning_rate,
        algo_target_update_interval=algo_target_update_interval,
        algo_scaler=algo_scaler,
        optimizer=optimizer,
        optimizer_weight_decay=optimizer_weight_decay,
        maxlen_buffer=maxlen_buffer,
        explorer_start_epsilon=explorer_start_epsilon,
        explorer_end_epsilon=explorer_end_epsilon,
        explorer_duration=explorer_duration,
    )
    # Launch training.
    rl_algo.fit_online(
        env,  # Gym environment.
        buffer,  # Buffer.
        explorer,  # Explorer.
        n_steps=training_steps,  # Train for 'training_steps' steps.
        n_steps_per_epoch=n_steps_per_epoch,  # Evaluation is performed every 'n_steps_per_epoch' steps.
        update_start_step=update_start_step,  # Parameter update starts after 'update_start_step' steps.
        save_metrics=save_metrics,  # Save metrics.
        show_progress=show_progress,  # Show progress.
    )
